{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2\n",
    "\n",
    "Problems are set by Dr. Wei Dai and Miss Farwa Abbas, 2022\n",
    "\n",
    "Instructions:\n",
    "\n",
    "While submitting your coursework please ensure that you rename the file as `Coursework_x_Group_x.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, FFTW, Wavelets, Images, LinearAlgebra, Random, StatsBase\n",
    "\n",
    "# Make sure that you complete the involvement table. \n",
    "# The first row is for CID number. \n",
    "# Other rows are for the involvement for each \"big\" problem (8 big problems in coursework 1). \n",
    "# \"1\" for getting involved in this part. \n",
    "# \"0\" for not involved\n",
    "Contributions = DataFrame( A=[1234,0,0,0,0], B = [1234,0,0,0,0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (6%)\n",
    "\n",
    "1. Let $A,B$ be two convex sets. Define $C:= A+B = \\left\\{ \\boldsymbol{a} + \\boldsymbol{b} |~ \\boldsymbol{a} \\in A,~ \\boldsymbol{b} \\in B \\right\\}$. Show that $C$ is convex.\n",
    "2. Let $A_k$, $k=1,2,\\cdots,K$, be convex sets. Show that $A := \\bigcap_{k=1}^K A_k$ is convex. \n",
    "3. Show that a set is convex if and only if its intersection with any line is convex. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (3%)\n",
    "\n",
    "This question is about the distance between two parallel hyperplanes $\\left\\{ \\boldsymbol{x} \\in \\mathbb{R}^n | \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} = b_1 \\right\\}$ and $\\left\\{ x \\in \\mathbb{R}^n | \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} = b_2 \\right\\}$. \n",
    "\n",
    "1.  Give a scientifically reasonable definition for the distance. \n",
    "2. Find the distance in a closed form. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 (12%)\n",
    "\n",
    "For each of the following sets, indicate whether it is convex or not and prove your answer.\n",
    "\n",
    "1.  A slab, i.e., $\\{ \\boldsymbol{x} \\in \\mathbb{R}^n  | \\alpha \\le \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} \\le \\beta \\}$. \n",
    "        \n",
    "2. A rectangle, i.e., $\\{ \\boldsymbol{x} \\in \\mathbb{R}^n  | \\alpha_i \\le x_i  \\le \\beta_i \\}$.\n",
    "        \n",
    "3. The set of points closer to a given point $x_0$ than to another given point $\\boldsymbol{y}$, i.e., \n",
    "        $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\lVert \\boldsymbol{x} - x_0 \\rVert _2 \\le \n",
    "                \\lVert \\boldsymbol{x} - \\boldsymbol{y} \\rVert _2\n",
    "            \\right\\}.\n",
    "        \\end{align*}$$\n",
    "        \n",
    "4. The set of points closer to a given point than to a given set, i.e., $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\lVert \\boldsymbol{x} - x_0 \\rVert _2 \\le \n",
    "                \\lVert \\boldsymbol{x} - \\boldsymbol{y} \\rVert _2,\n",
    "                ~ \\forall \\boldsymbol{y} \\in S\n",
    "            \\right\\}\n",
    "        \\end{align*}$$\n",
    "        where $S \\subseteq \\mathbb{R}^n$. \n",
    "        \n",
    "5. The set of points closer to one set than another, i.e., $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\text{dist}(\\boldsymbol{x},S) \\le \\text{dist}(\\boldsymbol{x},T)\n",
    "            \\right\\}\n",
    "        \\end{align*}$$\n",
    "        where $$\\begin{align*}\n",
    "            \\text{dist}(\\boldsymbol{x},S) := \\text{inf}\\{ \\lVert x - \\boldsymbol{z} \\rVert_2 ~\\mid~ \n",
    "            \\boldsymbol{z} \\in S \\}. \n",
    "        \\end{align*}$$\n",
    "\n",
    "6. The set of points whose distance to $\\boldsymbol{a}$ does not exceed a fixed fraction $\\theta$ of the distance to $\\boldsymbol{b}$, i.e., the set $\\{ \\boldsymbol{x} \\mid ~ \\lVert \\boldsymbol{x} - \\boldsymbol{a} \\rVert_2 \\le \\theta \\lVert \\boldsymbol{x} - \\boldsymbol{b} \\rVert_2 \\}$, where $\\boldsymbol{a} \\ne \\boldsymbol{b}$ and $0 \\le \\theta \\le 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Functions\n",
    "\n",
    "### 4 (5%)\n",
    "\n",
    "Use the second-order condition of convexity to prove the following functions are convex \n",
    "\n",
    "1.  $f(x) = - \\log(x)$ where $x \\in \\mathbb{R}^+$.\n",
    "2. $f(x) = x \\log (x)$ where $x \\in \\mathbb{R}^+$.\n",
    "3. Affine functions $f(\\boldsymbol{x}) = \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b}$. \n",
    "4. Quadratic functions $f(\\boldsymbol{x}) = \\frac{1}{2} \\boldsymbol{x}^{\\mathsf{T}} \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b}^{\\mathsf{T}} \\boldsymbol{x} + c$ where $\\boldsymbol{A} \\succeq 0$. \n",
    "5. $f(\\boldsymbol{x}) = \\frac{1}{2} \\lVert \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b} \\rVert_2^2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 (8%)\n",
    "\n",
    "1.  Prove that if $f(\\boldsymbol{x})$ is convex, then $g(\\boldsymbol{x}) := f(\\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b})$ is convex. \n",
    "        \n",
    "2.  Prove that any norm $\\lVert \\cdot \\rVert$ on $\\mathbb{R}^m$ is convex. \n",
    "        \n",
    "3. Let $$\\begin{align*}\n",
    "            f(\\boldsymbol{x}) := \\max_{i=1,\\cdots,k}~ \n",
    "            \\lVert  \\boldsymbol{A}^{(i)} \\boldsymbol{x} + \\boldsymbol{b}^{(i)} \\rVert, \n",
    "        \\end{align*}$$\n",
    "    where $\\boldsymbol{A}^{(i)} \\in \\mathbb{R}^{m \\times n}$, $\\boldsymbol{b}^{(i)} \\in \\mathbb{R}^m$, and $\\lVert \\cdot \\rVert$ is a norm on $\\mathbb{R}^m$. Indicate whether $f(\\cdot)$ is convex or not. Prove your claim. \n",
    "        \n",
    "4. Let $$\\begin{align*}\n",
    "            f(\\boldsymbol{x}) = \\sum_{i=1}^{r} |x|_{[i]},\n",
    "        \\end{align*}$$\n",
    "    where $\\vert x \\vert_{[i]}$ is the $i$ th largest component of $|x_1|, \\cdots, |x_n|$. Decide whether $f(\\cdot)$ is convex or not. Prove your claim. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Search\n",
    "\n",
    "### 6 (6%)\n",
    "Let $\\boldsymbol{x} \\in \\mathbb{R}^2$ and \n",
    "$$\\begin{align*}\n",
    "        f(\\boldsymbol{x}) = 3 |x_1| +  |x_2|.\n",
    "\\end{align*}$$\n",
    "Consider the point $\\boldsymbol{y} = [0,1]^{\\mathsf{T}}$. \n",
    "\n",
    "1.  Show that $\\boldsymbol{g} = [3,1]^{\\mathsf{T}} \\in \\partial f(\\boldsymbol{y})$. \n",
    "2.  Let $\\tau \\in (0,1)$, find the closed form for \n",
    "    $$\\begin{align*}\n",
    "        f(\\boldsymbol{y} - \\tau \\boldsymbol{g} ).\n",
    "    \\end{align*}$$\n",
    "3. Comment on whether $-\\boldsymbol{g}$ is a descent direction or not.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "$$\n",
    "\\begin{align*}\n",
    "   \\partial f(\\boldsymbol{y}) = [\\frac{d}{dy_1} \\boldsymbol{y} , \\frac{d}{dy_2} \\boldsymbol{y} ]^T = [\\frac{d}{dy_1} ( 3 |x_1| +  |x_2|) , \\frac{d}{dy_2} ( 3 |x_1| +  |x_2|) ]^T = [3, 1]^T =  \\boldsymbol{g}  \n",
    "\\end{align*}\n",
    "$$\n",
    "2.\n",
    "$$\n",
    " f(\\boldsymbol{y} - \\tau \\boldsymbol{g} ) = 3 |y_1-\\tau g_1| +  |y_2 -\\tau g_2| = 3|0-\\tau 3| + |1-\\tau 1| \\\\\n",
    " \\text{Since $\\tau$ is positive: }\\\\\n",
    " f(\\boldsymbol{y} - \\tau \\boldsymbol{g} ) = 9\\tau + 1-\\tau = 8\\tau +1\n",
    "$$\n",
    "3.\n",
    "$$\n",
    "\\text{As we proceed in the -g direction, the output increases, indicating that -g is an ascending gradient. }\\\\\n",
    "\\text{To prove this: }\\\\\n",
    "\\text{for -g to be a descending gradient, we have: }\\\\\n",
    "f(\\boldsymbol{y} - \\tau \\boldsymbol{g} ) < f(\\boldsymbol{y}) \\\\\n",
    "8\\tau +1 < 1 \\\\\n",
    "8\\tau<0\\\\\n",
    "\\text{Since }\\tau \\in (0,1) \\text{, the condition cannot be met; therefore, -g is ascending.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 (10%)\n",
    "\n",
    "The following is the famous Wolfe's example, which shows that gradient descent method may not converge to a local optimal point.\n",
    "Let $\\boldsymbol{x}\\in \\mathbb{R}^2$ and \n",
    "$$\\begin{align*}\n",
    "        f(\\boldsymbol{x}) = \\begin{cases}\n",
    "            5(9x_1^2 + 16 x_2^2)^{1/2} & \\text{if}~ x_1 > |x_2|, \\\\\n",
    "            9x_1 + 16|x_2| & \\text{if}~ x_1 \\le |x_2|.\n",
    "        \\end{cases}\n",
    "    \\end{align*}$$\n",
    "Suppose that $\\boldsymbol{x}^0 = [16/9,1]^{\\mathsf{T}}$. Consider exact line search where \n",
    "    $$\\begin{align*}\n",
    "        \\boldsymbol{x}^{l+1} = \\boldsymbol{x}^l - t^{l+1} \\nabla f(\\boldsymbol{x}^l),\n",
    "    \\end{align*}$$\n",
    "    where \n",
    "    $$\\begin{align*}\n",
    "        t^{l+1} = \\arg~ \\min_t~ f(\\boldsymbol{x}^l - t \\nabla f(\\boldsymbol{x}^l)).\n",
    "    \\end{align*}$$\n",
    "\n",
    "\n",
    "1. Draw the contours of $f(\\boldsymbol{x})$ in the region $-2 \\le x_1 \\le 2$ and $-2 \\le x_2 \\le 2$.\n",
    "2. Is the point $\\boldsymbol{x} = [0,0]^{\\mathsf{T}}$ optimal? Why?\n",
    "3. Find the closed form of $\\nabla f(\\boldsymbol{x})$ in the region where $x_1 > |x_2|$.\n",
    "4.  Find $t^1$ and $\\boldsymbol{x}^1$. \n",
    "5. Find $t^2$ and $\\boldsymbol{x}^2$.\n",
    "6. Use mathematical induction, find $t^l$ and $\\boldsymbol{x}^l$. It can be concluded that $\\boldsymbol{x}^l \\rightarrow [0,0]^{\\mathsf{T}}$ as $l \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Gradient Method\n",
    "\n",
    "### 8 (12%)\n",
    "For a given function $f$ and a given constant $\\gamma >0$, the proximal operator is given by\n",
    "\n",
    "$$\\begin{equation*}\n",
    "    \\bm{x}^{\\star} = \\text{Prox}_{\\gamma f}(\\bm{z}) := \\arg~ \n",
    "    \\min_{\\bm{x}}~ f(\\bm{x}) \n",
    "    + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2.\n",
    "\\end{equation*}$$\n",
    "\n",
    "Find the closed form of the proximal operator $\\text{Prox}_{\\gamma f}(\\bm{z})$ for the following functions $f$. Give the key steps for your derivation.  \n",
    "1. $f(\\bm{x}) = \\delta( \\bm{x} \\le u ) $ (each element of $x$ is at most $u$). \n",
    "2. $f(\\bm{x}) = \\delta( l \\le \\bm{x} \\le u ) $ (each element of $x$ is in $[l,u]$).\n",
    "3. $f(\\bm{x}) = \\delta( \\lVert \\bm{x} \\rVert_{\\infty} \\le a ) $. (See Lecture Notes Example 5.1 for the definition of $\\lVert \\cdot \\rVert_{\\infty}$). \n",
    "4. $f(\\bm{x}) = \\frac{1}{2} \\lVert \\bm{A} \\bm{x} - \\bm{b} \\rVert^2 $. \n",
    "5. $f(\\bm{x}) = \\lVert \\bm{x} \\rVert$. \n",
    "6. $f(\\bm{x}) = \\delta( \\lVert \\bm{x} \\rVert \\le a )$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For:\n",
    "$$\n",
    "    \\text{Prox}_{\\gamma f}(\\bm{z}) := \\arg~ \n",
    "    \\min_{\\bm{x}}~ \\delta( \\bm{x} \\le u )  \n",
    "    + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\\n",
    "$$\n",
    "If  $\\bm{x} \\le u $, then $\\delta( \\bm{x} \\le u )=0 $ and:\n",
    "$$    \n",
    "    \\text{Prox}_{\\gamma f}(\\bm{z}) := \\arg~ \\min_{\\bm{x}}~ \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "$$\n",
    "Therefore: $ \\bm{z} =\\bm{x}$\n",
    "Otherwise, if  $\\bm{x} > u $, then the delta function has infinite cost.  \n",
    "As proven in (3), a slab ($\\bm{x} > u $) is a convex set; thus, the proximal operator can be expressed in terms of the projection matrix back into the convex set, corresponding to: $\\bm{z} = u$ .\n",
    "Therefore, $\\forall i \\in \\bm{z}$: \n",
    "$$\\begin{align*}\n",
    "        \\Pi(z_i) = \\begin{cases}\n",
    "            z_i & \\text{if}~ z_i \\le u , \\\\\n",
    "            u & \\text{if}~z_i > u .\n",
    "        \\end{cases}\n",
    "\\end{align*}$$\n",
    "\n",
    "2. \n",
    "$$\\begin{align*}\n",
    "         \\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \\min_{\\bm{x}}~ \\delta( l \\le \\bm{x} \\le u )  \n",
    "        + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \n",
    "\\end{align*}$$\n",
    "Following the same reasoning as per (1), since the intersection of two convex sets ($ l \\le z_i \\le u $) is a convex set, we can express the proximal operator as the projection matrix on the convex set and get $\\forall i \\in \\bm{z}$:\n",
    "$$\\begin{align*}\n",
    "         \\Pi(z_i) = \\begin{cases}\n",
    "            l & \\text{if}~ z_i \\le l , \\\\\n",
    "            z_i & \\text{if}~ l \\le z_i \\le u \\\\\n",
    "            u & \\text{if}~ z_i > u .\n",
    "        \\end{cases}\n",
    "\\end{align*}$$\n",
    "\n",
    "3. \n",
    "$$\\begin{align*}\n",
    "        \\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \\min_{\\bm{x}}~  \\delta( \\lVert \\bm{x} \\rVert_{\\infty} \\le a )\n",
    "        + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2\n",
    "\\end{align*}$$\n",
    "\n",
    "Following the same reasoning as per (8.1,8.2) and all l norms are convex set (proven in (5)), given that $\\lVert \\bm{z} \\rVert_{\\infty} = z_m$ with $ |z_m| > |z_n|  \\forall{n} \\in \\bm{z}$ with $n \\ne m$ , we get $\\forall i \\in \\bm{z}$:\n",
    "$$\\begin{align*}\n",
    "        \\Pi(z_i) = \\begin{cases}\n",
    "            z_i & \\text{if}~ |z_i| = |z_m |\\le a , \\\\\n",
    "            a & \\text{if}~ z_i > a , \\\\\n",
    "            -a & \\text{if}~ z_i < a. \\\\\n",
    "        \\end{cases}\n",
    "\\end{align*}$$\n",
    "4. \n",
    "$$\\begin{align*}\n",
    " \\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "    \\min_{\\bm{x}}~ \\frac{1}{2} \\lVert \\bm{A} \\bm{x} - \\bm{b} \\rVert^2 + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "    = \\arg~ \\min_{\\bm{x}}~ \n",
    "    \\frac{1}{2} \\bm{A^TA} \\bm{x^2} - \\bm{Abx} +\\frac{1}{2}\\bm{b^2}  + \\frac{1}{2 \\gamma} \\bm{x^2} - \\frac{1}{\\gamma}\\bm{xz}+\\frac{1}{2 \\gamma} \\bm{z^2}\\\\\n",
    "    \\text{To find the minimum we differentiate, getting:}\\\\\n",
    "    \\bm{A^TA} \\bm{x} - \\bm{Ab}   + \\frac{1}{ \\gamma} \\bm{x} - \\frac{1}{\\gamma}\\bm{z} = 0\\\\\n",
    "    \\text{ Since the square is always positive and }\\gamma >0 \\text{ by definition,}\\\\\n",
    "    \\text{the second derivative is positive: }  \\bm{A^TA} + \\frac{1}{ \\gamma}> 0\n",
    "    \\text{ and, thus, this is a minimum.}\\\\\n",
    "    \n",
    "    \\text{Therefore: }\\\\\n",
    "    \\text{Prox}_{\\gamma f}(\\bm{z}) = \\bm{x}(\\bm{A^TA}\\gamma +1)-\\gamma \\bm{Ab}\n",
    "\\end{align*}$$\n",
    "5. \n",
    "$$\\begin{align*}\n",
    "    \\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "    \\min_{\\bm{x}}~\\lVert \\bm{x} \\rVert + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "    = \\arg~ \n",
    "    \\min_{\\bm{x}}~\\lVert \\bm{x} \\rVert + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \n",
    "\\end{align*}$$\n",
    "Partitioning the problem $\\forall i \\in \\bm{x}$:\n",
    "$$\\begin{align*}\n",
    "    \\text{Prox}_{\\gamma f}(z_i) = \\arg~ \n",
    "    \\min_{\\bm{x}}~2\\gamma|x_i| + x_i^2 - 2z_i x_i + z_i^2 = \\\\\n",
    "     = \\begin{cases}\n",
    "            \\gamma -z_i & \\text{if}~ z_i > 0 , \\\\\n",
    "            \\gamma + z_i & \\text{if}~ z_i < 0 , \\\\\n",
    "            -1 & \\text{if}~ z_i = 0^- , \\\\\n",
    "            1 & \\text{if}~ z_i = 0^+. \\\\\n",
    "        \\end{cases}\\\\\n",
    "    \\text{Therefore: } \\text{Prox}_{\\gamma f}(\\bm{z}) = sign(z_i)max(|z_i|-\\gamma, 0)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Similarly to (8.3), ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 (14%)\n",
    "Suppose that we know how to compute the proximal operator $\\text{Prox}_{\\gamma g}(\\bm{z})$. Find the form of the proximal operator $\\text{Prox}_{\\gamma f}(\\bm{z})$ in terms of $\\text{Prox}_{\\gamma g}(\\bm{z})$ for the following functions $f$. Give the key steps for your derivation. \n",
    "\n",
    "1.  $f(\\bm{x}) = g(\\bm{x}) + \\frac{\\rho}{2} \\lVert \\bm{x} - \\bm{b} \\rVert^2$ where $\\rho > 0$. \n",
    "2.  $f(\\bm{x}) = g(\\bm{x}) + \\langle \\bm{a},\\bm{x} \\rangle$.\n",
    "3.  $f(\\bm{x}) = a g(\\bm{x}) + b$ where $a>0$.\n",
    "4.  $f(\\bm{x}) = g(\\bm{x} + b)$.\n",
    "5.  $f(\\bm{x}) = g(a\\bm{x})$ where $a>0$.\n",
    "6.  $f(\\bm{x}) = g(\\bm{A}\\bm{x})$ where $\\bm{A}$ is orthonormal, i.e., $\\bm{A}^{\\mathsf{T}}\\bm{A} = \\bm{A} \\bm{A}^{\\mathsf{T}} = \\bm{I}$.\n",
    "7.  $f(\\bm{x}) = g(a\\bm{A}\\bm{x})$ where $a>0$ and $\\bm{A}$ is orthonormal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "$$\\begin{align*}\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "\\min_{\\bm{x}}~ g(\\bm{x}) + \\frac{\\rho}{2} \\lVert \\bm{x} - \\bm{b} \\rVert^2+ \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "\\text{Since the proximal operator of g is known, we can scompone the problem in the following way: }\\\\\n",
    "\\text{1. Find the condition on z that get the minimum of the second and third term}\\\\\n",
    "\\text{2. Evaluate the proximal operator of g on the on the z found}\\\\\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\text{Prox}_{\\gamma g}(\\min_{\\bm{x}}~ \\frac{\\rho}{2} \\lVert \\bm{x} - \\bm{b} \\rVert^2+ \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2) \\\\\n",
    "1. \\text{ Differentiating over x we get: } \\bm{x_{min}} =  \\frac{\\gamma \\rho \\bm{b}}{\\gamma \\rho +1} + \\frac{\\bm{z}}{\\gamma \\rho +1}\\\\\n",
    "2. \\text{ Then: } \\text{Prox}_{\\gamma f}(\\bm{z}) =\\text{Prox}_{\\frac{\\gamma}{1+\\rho \\gamma} g}(\\frac{\\gamma \\rho \\bm{b}}{\\gamma \\rho +1} + \\frac{\\bm{z}}{\\gamma \\rho +1})\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "\\min_{\\bm{x}}~ g(\\bm{x}) + \\frac{\\rho}{2} \\lVert \\bm{x} - \\bm{b} \\rVert^2+ \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "\\arg~ \\min_{\\bm{x}}~ \\frac{1}{2}((\\frac{1}{\\gamma}+\\rho)x^Tx- 2(\\frac{z}{\\gamma}+\\rho b)^Tx + (z^Tz+\\rho a^Ta)) + g(u)\\\\\n",
    "arg~ \\min_{\\bm{x}}~ \\frac{1}{2} \\|x - \\frac{\\frac{z}{\\gamma}+\\rho b}{\\frac{1}{\\gamma}+\\rho} \\|^2 +\\frac{1}{\\frac{1}{\\gamma}+\\rho} g(z) \\\\\n",
    "arg~ \\min_{\\bm{x}}~ \\frac{1}{2} \\|x - \\frac{\\frac{z}{\\gamma}+\\rho b}{\\frac{1}{\\gamma}+\\rho} \\|^2+\\frac{1}{\\frac{1}{\\gamma}+\\rho} g(z) \\\\\n",
    "\\lambda = 1/(\\frac{1}{\\gamma}+\\rho) \\\\\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\text{Prox}_{\\lambda g}((\\frac{z}{\\gamma}+\\rho b)\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "\\min_{\\bm{x}}~ g(\\bm{x}) + <\\bm{a}, \\bm{x}> + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "\\text{For the same rationale as in (9.1) we get: }\\\\\n",
    "1. \\text{ Differentiating over x we get: }  \\frac{d}{dx}(<\\bm{a}, \\bm{x}> + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2)\\\\\n",
    "\\bm{a}^T + \\frac{1}{\\gamma} \\bm{x}-\\frac{\\bm{z}}{\\gamma}\\\\\n",
    "\\bm{x_{min}} = \\bm{z}-\\gamma\\bm{a}^T = 0\\\\\n",
    "2. \\text{ Then: } \\text{Prox}_{\\gamma f}(\\bm{z}) =\\text{Prox}_{\\gamma g}(\\bm{z}-\\gamma\\bm{a}^T)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  \n",
    "$$\\begin{align*}\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "\\min_{\\bm{x}}~ a g(\\bm{x}) + b + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \n",
    "\\text{ with $a>0$}\\\\\n",
    "\\text{We can neglect the term b since it does not depend on x ($\\frac{d}{dx} x+b =\\frac{d}{dx} x$)}\\\\\n",
    "\\text{Thus, }\n",
    "\\arg~ \n",
    "\\min_{\\bm{x}}~ a g(\\bm{x}) + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 =\\text{Prox}_{\\gamma h}(\\bm{z}) \n",
    "\\text{ with } h(\\bm{x})= ag(\\bm{x}) \\text{ by definition.}\\\\\n",
    "\\text{ Therefore: }  \\text{Prox}_{\\gamma f}(\\bm{z})  = \\text{Prox}_{\\gamma h}(\\bm{z})  = \\text{Prox}_{\\gamma ag}(\\bm{z})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.   \n",
    "$$\\begin{align*}\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "\\min_{\\bm{x}}~ g(\\bm{x} + b) + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "\\text{Rearranging the equation for $\\bm{y}=\\bm{x}+b$ we get:}\\\\\n",
    "\\bm{y*} = \\arg~ \\min_{\\bm{x}}~ g(\\bm{y}) + \\frac{1}{2 \\gamma} \\left\\| \\bm{y}-b - \\bm{z} \\right\\|_2^2 = \\text{Prox}_{\\gamma g}(\\bm{z+b}) \\text{ as proven in (9.3)} \\\\\n",
    "\\text{ Since }\n",
    "\\bm{y*} =  \\text{Prox}_{\\gamma f}(\\bm{z})+b\\\\\n",
    "\\text{ Thus, }\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z})  = \\text{Prox}_{\\gamma g}(\\bm{z+b})-b \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.   \n",
    "$$\\begin{align*}\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "\\min_{\\bm{x}}~ g(a\\bm{x}) + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "\\text{Rearranging the equation for $\\bm{y}=a\\bm{x}$ we get:}\\\\\n",
    "\\bm{y*} = \\arg~ \\min_{\\bm{x}}~ g(\\bm{y}) + \\frac{1}{2 \\gamma} \\left\\| \\frac{\\bm{y}}{a} - \\bm{z} \\right\\|_2^2 = \\text{Prox}_{a^2\\gamma g}(a\\bm{z}) \\text{ as proven in (9.3)} \\\\\n",
    "\\text{ Since }\n",
    "\\bm{y*} =  \\frac{1}{a}\\text{Prox}_{\\gamma f}(\\bm{z})\\\\\n",
    "\\text{ Thus, }\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z})  = \\frac{1}{a}\\text{Prox}_{a^2\\gamma g}(\\bm{az}) \n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.    \n",
    "$$\\begin{align*}\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "\\min_{\\bm{x}}~ g(\\bm{A}\\bm{x}) + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "\\text{For the rule proven in  (9.5), we get:}\\\\\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z})  = \\frac{1}{\\bm{A}} \\text{Prox}_{A^TA\\gamma g}(\\bm{Az})\\\\\n",
    "\\text{Since A is orthonormal ($\\bm{A^T\\bm{A} = I}$) and $\\frac{1}{\\bm{A}}= \\bm{A}^T$, we then get:}\\\\\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) =\\bm{A}^T \\text{Prox}_{\\gamma g}(\\bm{Az}) \n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.    \n",
    "$$\\begin{align*}\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z}) = \\arg~ \n",
    "\\min_{\\bm{x}}~ g(a\\bm{A}\\bm{x}) + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2 \\\\ \n",
    "\\text{Combining the proofs in (9.5, 9.6) we get:}\\\\\n",
    "\\text{Prox}_{\\gamma f}(\\bm{z})  = \\frac{1}{a} \\bm{A}^T \\text{Prox}_{a^2\\gamma g}(\\bm{aAz}) \n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###   10 MRI Compressed Sensing Recovery via Proximal Gradient (24%)\n",
    "\n",
    "Let $\\bm{X}$ be an MRI image. A typical MRI image is sparse under wavelet transform. An MRI machine takes samples in the frequency domain. \n",
    "\n",
    "\n",
    "*  We verify that an MRI image is sparse under wavelet transform as follows. (6%)\n",
    "\n",
    "    1.  Use the package `Images` to load `MRI_of_Human_Brain.jpg`. Show the MRI image that you load. Denote the image by `X0`. \n",
    "\n",
    "    2.  Use the package `Wavelets` to perform 2D Daubechies wavelet transform of the MRI image. Show the wavelet coefficients as an image. (See `Github>Wavelets.jl/example/transform2d` for an example.) Denote the wavelet coefficients by `W0`.\n",
    "    \n",
    "    3.  Keep 20% wavelet coefficients that are of the largest magnitudes and set the rest wavelet coefficients to zero. Denote the resulting 'truncated' wavelet coefficients by `W1`. Perform inverse 2D Daubechies wavelet transform to obtain an image `X1`. This image should look like the image `X0` with minor quality loss. \n",
    "\n",
    "    \n",
    "* Fourier domain compressed sensing: (6%)\n",
    "\n",
    "    4.  Use the package `FFTW` to perform 2D Fourier transform of `X0`. Denote the obtained Fourier coefficients by `F0`. \n",
    "\n",
    "    5. Set the random seed to 0. Generate the random sampling index set by using `StatsBase.sample` and denote it by $\\Omega$ such that the size of $\\Omega$ is 30% of the size of `X0`. \n",
    "\n",
    "    6.  Let $y = \\mathcal{P}_{\\Omega} ( \\mathcal{F}(X0))$. It is our MRI measurement vector. \n",
    "\n",
    "\n",
    "* Compressed sensing recovery. We are going to use proximal gradient method to solve the optimization problem. We are going to denote the recovered image by `Xhat`. (12%) \n",
    "\n",
    "    $$\\begin{align*}\n",
    "        & \\min_{X}~ F(X) := \\lambda \\lVert \\mathcal{W}(X) \\rVert_1 + \\frac{1}{2} \\lVert y - \\mathcal{P}_{\\Omega} (\\mathcal{F}(X)) \\rVert_2^2.\n",
    "    \\end{align*}$$ \n",
    "    \n",
    "    7.  Use the results that you obtained in coursework problem related to proximal operators to obtain the form of the proximal operator for the function $g(X) = \\lambda \\lVert \\mathcal{W}(X) \\rVert_1$. \n",
    "\n",
    "    8.  Write the objective function $F(X) = f(X) + g(X)$ where $f(X)$ is differentiable. Find the closed form for $\\nabla f(X)$ (Recall the adjoint of linear operator $\\mathcal{A}$ is denoted by $\\mathcal{A}^*$). \n",
    "    \n",
    "    9.  Implement the proximal gradient method to recover the image from $y$. In your codes, use comments to indicate clearly which part is to compute $\\nabla f(X)$, which part is to perform proximal operator, and which part is to find a valid step size. \n",
    "\n",
    "\n",
    "    10. Test the performance. Now find a good value of $\\lambda$ so that the recovery result looks nice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0de548ffb181fe2a01b2ed5f402e6bb6e6ccf2641efffae0817d36d41e1e10e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

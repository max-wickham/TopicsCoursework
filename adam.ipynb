{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2\n",
    "\n",
    "Problems are set by Dr. Wei Dai and Miss Farwa Abbas, 2022\n",
    "\n",
    "Instructions:\n",
    "\n",
    "While submitting your coursework please ensure that you rename the file as \"Coursework_x_Group_x.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, FFTW, Wavelets, Images, LinearAlgebra, Random, StatsBase\n",
    "\n",
    "# Make sure that you complete the involvement table. \n",
    "# The first row is for CID number. \n",
    "# Other rows are for the involvement for each \"big\" problem (8 big problems in coursework 1). \n",
    "# \"1\" for getting involved in this part. \n",
    "# \"0\" for not involved\n",
    "Contributions = DataFrame( A=[1234,0,0,0,0], B = [1234,0,0,0,0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (6%)\n",
    "\n",
    "1. Let $A,B$ be two convex sets. Define $C:= A+B = \\left\\{ \\boldsymbol{a} + \\boldsymbol{b} |~ \\boldsymbol{a} \\in A,~ \\boldsymbol{b} \\in B \\right\\}$. Show that $C$ is convex.\n",
    "2. Let $A_k$, $k=1,2,\\cdots,K$, be convex sets. Show that $A := \\bigcap_{k=1}^K A_k$ is convex. \n",
    "3. Show that a set is convex if and only if its intersection with any line is convex. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "Let $c,c'\\in A+B$. Let $a,a'\\in A$ and $b,b'\\in B$ such that $c=a+b$ and $c'=a'+b'$.\n",
    "\n",
    "If $\\lambda\\in[0,1]$, $\\lambda c+(1-\\lambda)c'=(\\lambda a+(1-\\lambda)a')+(\\lambda b+(1-\\lambda)b')$. \n",
    "\n",
    "Because $A$ is convex, $\\lambda a+(1-\\lambda)a'\\in A$ and because $B$ is convex, $\\lambda b+(1-\\lambda)b'\\in B$.\n",
    "\n",
    "Therefore, $\\lambda c+(1-\\lambda)c'\\in A+B$ which proves that $A+B$ is convex.\n",
    "\n",
    "2. \n",
    "For any $ x,y \\in \\cap_{k=1}^K A_k $, $ t \\in [0,1] $:\n",
    "\n",
    "$ tx + (1-t)y \\in A_k $ by convexity of $ A_k $ (using answer (1a))\n",
    "\n",
    "Therefore, $ tx + (1-t)y \\in \\cap_{k=1}^K A_k $\n",
    "\n",
    "Thus, $ \\cap_{k=1}^K A_k $ is convex.\n",
    "\n",
    "3. \n",
    "Let $ x , y \\epsilon $ C and let $ \\lambda \\epsilon [0,1]. $ We wish to prove $ \\lambda x + (1−\\lambda)y $ $ \\epsilon $ C. \n",
    "\n",
    "Let L be the line through $ x $ and $ y $. $ x $ and $ y $ are in C∩L. By convexity of C∩L, we have that $ \\lambda x + (1−\\lambda)y $ $ \\epsilon $ C∩L and therefore $ \\lambda x + (1−\\lambda)y $ $ \\epsilon $ C. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (3%)\n",
    "\n",
    "This question is about the distance between two parallel hyperplanes $\\left\\{ \\boldsymbol{x} \\in \\mathbb{R}^n | \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} = b_1 \\right\\}$ and $\\left\\{ x \\in \\mathbb{R}^n | \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} = b_2 \\right\\}$. \n",
    "\n",
    "1.  Give a scientifically reasonable definition for the distance. \n",
    "2. Find the distance in a closed form. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "The distance between the two hyperplanes is the distance between two points $ x_1 $ and $ x_2 $ such that the hyperplane intersects the line through the origin and parallel to normal vector $ \\bm{a} $. \n",
    "\n",
    "2.\n",
    "The points $ x_1 $ and $ x_2 $ described above are given by: \n",
    "\n",
    "$ x_1 = (\\frac{b_1}{\\|a\\|_2^2})a $, $ x_2 = (\\frac{b_2}{\\|a\\|_2^2})a $.\n",
    "\n",
    "Therefore, the distance between them is: \n",
    "\n",
    "$ \\| x_1 - x_2 \\|$ = \n",
    "$ (\\frac{b_1}{\\|a\\|_2^2})a - (\\frac{b_2}{\\|a\\|_2^2})a $\n",
    "$ = \\frac{|b_1 - b_2|}{\\|a\\|_2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 (12%)\n",
    "\n",
    "For each of the following sets, indicate whether it is convex or not and prove your answer.\n",
    "\n",
    "1.  A slab, i.e., $\\{ \\boldsymbol{x} \\in \\mathbb{R}^n  | \\alpha \\le \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} \\le \\beta \\}$. \n",
    "        \n",
    "2. A rectangle, i.e., $\\{ \\boldsymbol{x} \\in \\mathbb{R}^n  | \\alpha_i \\le x_i  \\le \\beta_i \\}$.\n",
    "        \n",
    "3. The set of points closer to a given point $x_0$ than to another given point $\\boldsymbol{y}$, i.e., \n",
    "        $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\lVert \\boldsymbol{x} - x_0 \\rVert _2 \\le \n",
    "                \\lVert \\boldsymbol{x} - \\boldsymbol{y} \\rVert _2\n",
    "            \\right\\}.\n",
    "        \\end{align*}$$\n",
    "        \n",
    "4. The set of points closer to a given point than to a given set, i.e., $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\lVert \\boldsymbol{x} - x_0 \\rVert _2 \\le \n",
    "                \\lVert \\boldsymbol{x} - \\boldsymbol{y} \\rVert _2,\n",
    "                ~ \\forall \\boldsymbol{y} \\in S\n",
    "            \\right\\}\n",
    "        \\end{align*}$$\n",
    "        where $S \\subseteq \\mathbb{R}^n$. \n",
    "        \n",
    "5. The set of points closer to one set than another, i.e., $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\text{dist}(\\boldsymbol{x},S) \\le \\text{dist}(\\boldsymbol{x},T)\n",
    "            \\right\\}\n",
    "        \\end{align*}$$\n",
    "        where $$\\begin{align*}\n",
    "            \\text{dist}(\\boldsymbol{x},S) := \\text{inf}\\{ \\lVert x - \\boldsymbol{z} \\rVert_2 ~\\mid~ \n",
    "            \\boldsymbol{z} \\in S \\}. \n",
    "        \\end{align*}$$\n",
    "\n",
    "6. The set of points whose distance to $\\boldsymbol{a}$ does not exceed a fixed fraction $\\theta$ of the distance to $\\boldsymbol{b}$, i.e., the set $\\{ \\boldsymbol{x} \\mid ~ \\lVert \\boldsymbol{x} - \\boldsymbol{a} \\rVert_2 \\le \\theta \\lVert \\boldsymbol{x} - \\boldsymbol{b} \\rVert_2 \\}$, where $\\boldsymbol{a} \\ne \\boldsymbol{b}$ and $0 \\le \\theta \\le 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Slab is convex.\n",
    "\n",
    "Let $ S = \\{\\bm{x} \\in \\R^n | \\alpha \\leq \\bm{a^Tx} \\leq \\beta \\} $\n",
    "\n",
    "Then $ S $ is the intersection of 2 Halfspaces $ H_1 $ and $ H_2 $:\n",
    "\n",
    "$ H_1 = \\{\\bm{x} \\in \\R^n | \\alpha \\leq \\bm{a^Tx}\\} $\n",
    "\n",
    "$ H_2 = \\{\\bm{x} \\in \\R^n | \\bm{a^Tx} \\leq \\beta\\} $\n",
    "\n",
    "Therefore, $ S = H_1 \\cap H_2 $.\n",
    "\n",
    "We can use our answer from (1a) whereby intersections of convex sets are convex to then say that S (i.e. a slab) is also convex. \n",
    "\n",
    "2.\n",
    "A rectange is convex because it is a convex quadrilateral. \n",
    "\n",
    "3. \n",
    "The set of points closer to a given point $ x_0 $ than to another given point $ y $ is convex because it can be proved to be a halfspace, which we have already proved as convex.\n",
    "\n",
    "As we are using the Euclidean norm to define distance, we can say that: $ \\|x-a\\|_2 \\leq \\|x-b\\|_2 $ if and only if $ \\|x-a\\|_2^2 \\leq \\|x-b\\|_2^2 $\n",
    "\n",
    "From this, $ \\|x-a\\|_2^2 \\leq \\|x-b\\|_2^2 = (x-a)^T(x-a) \\leq (x-b)^T(x-b) = x^Tx - 2a^Tx + a^Ta \\leq x^Tx - 2b^Tx + b^Tb = 2(b-a)^Tx \\leq b^Tb-a^Ta $\n",
    "\n",
    "Let $ c = 2(b-a) $ and $ d = b^Tb-a^Ta $. Then the final expression becomes: $ c^Tx \\leq d $ which is a halfspace. \n",
    "\n",
    "Thus, as a halfspace is convex, this is also convex.\n",
    "\n",
    "4.\n",
    "The set of points closer to a given point than to a given set is convex.\n",
    "\n",
    "This is because it can be expressed as follows: \n",
    "\n",
    "$ \\cap_{y \\in S} \\{x | \\|x-x_0\\|_2 \\leq \\|x-y\\|_2\\}$\n",
    "\n",
    "Which is an intersection of halfspaces. We know that halfspaces are convex (answer 3c) and that the intersection of convex sets is convex (answer 1b)\n",
    "\n",
    "Therefore, it is convex.\n",
    "\n",
    "5.\n",
    "The set of points closer to one set than another is not convex.\n",
    "\n",
    "We can prove this with an example: Let $ S = {-1,1} $ and $ T = {0} $.\n",
    "\n",
    "Now, $ \\{x | \\bm{dist}(x,S) \\leq \\bm{dist}(x,T)\\} = \\{x \\in \\R | x \\leq \\frac{-1}{2}, x \\geq \\frac{1}{2}\\} $\n",
    "\n",
    "Which is not convex.\n",
    "\n",
    "6.\n",
    "The set is convex.\n",
    "\n",
    "$$ \\{x |  \\|x-a\\|_2 \\leq \\theta\\|x-b\\|_2\\} $$ \n",
    "$$ = \\{x | \\|x-a\\|_2^2 \\leq \\theta^2\\|x-b\\|_2^2\\} $$\n",
    "$$ = \\{x | (1-\\theta^2)x^Tx - 2(a-\\theta^2b)^Tx + (a^Ta - \\theta^2b^Tb) \\leq 0 \\} (*)$$ \n",
    "\n",
    "$ 0 \\lt \\theta \\leq 1 $:\n",
    "When $ \\theta = 1 $: (*) becomes: \n",
    "$$ \\{x | -2(a-b)^Tx + (a^Ta - b^Tb) \\leq 0\\} $$\n",
    "$$ = \\{x | (a^Ta - b^Tb) \\leq 2(a-b)^Tx\\} $$\n",
    "We can use the method shown in (answer 3c) to show this is a halfspace, which is convex.\n",
    "\n",
    "When $ \\theta \\leq 1 $: (*) can be shown to be a ball, which is also convex.\n",
    "\n",
    "Therefore, the set is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Functions\n",
    "\n",
    "### 4 (5%)\n",
    "\n",
    "Use the second-order condition of convexity to prove the following functions are convex \n",
    "\n",
    "1.  $f(x) = - \\log(x)$ where $x \\in \\mathbb{R}^+$.\n",
    "2. $f(x) = x \\log (x)$ where $x \\in \\mathbb{R}^+$.\n",
    "3. Affine functions $f(\\boldsymbol{x}) = \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b}$. \n",
    "4. Quadratic functions $f(\\boldsymbol{x}) = \\frac{1}{2} \\boldsymbol{x}^{\\mathsf{T}} \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b}^{\\mathsf{T}} \\boldsymbol{x} + c$ where $\\boldsymbol{A} \\succeq 0$. \n",
    "5. $f(\\boldsymbol{x}) = \\frac{1}{2} \\lVert \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b} \\rVert_2^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "$ f(x) = -log(x) $ where $ x \\in \\R^+ $\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{-1}{x} $$\n",
    "\n",
    "$$ \\frac{\\partial^2 f}{\\partial x^2} = \\frac{1}{x^2} $$\n",
    "\n",
    "Therefore, $ \\bigtriangledown^2 f \\succeq 0$, so $f(x)$ is convex\n",
    "\n",
    "2.\n",
    "$ f(x) = xlog(x) $\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = log(x) + 1 $$\n",
    "\n",
    "$$ \\frac{\\partial^2 f}{\\partial x^2} = \\frac{1}{x} $$\n",
    "\n",
    "As $ x \\in \\R^+ $, $ \\bigtriangledown^2 f \\succeq 0$, so $f(x)$ is convex\n",
    "\n",
    "3.\n",
    "$ f(x) = \\bm{Ax + b} $\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\bm{A} $$\n",
    "\n",
    "$$ \\frac{\\partial^2 f}{\\partial x^2} = \\bm{I} $$\n",
    "\n",
    "Therefore, $ \\bigtriangledown^2 f \\succeq 0$, so $f(x)$ is convex\n",
    "\n",
    "4.\n",
    "$ f(x) = \\frac{1}{2}\\bm{x^TAx + b^Tx} + c $, $ \\bm{A} \\succeq 0 $.\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\bm{2Ax} $$\n",
    "\n",
    "$$ \\frac{\\partial^2 f}{\\partial x^2} = \\bm{2A} $$\n",
    "\n",
    "As $ A \\succeq 0 $, $ \\bigtriangledown^2 f \\succeq 0$, so $f(x)$ is convex\n",
    "\n",
    "5. \n",
    "$ f(x) = \\frac{1}{2} \\|\\bm{Ax+b}\\|_2^2 $\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\bm{A(Ax+b)} $$\n",
    "\n",
    "$$ \\frac{\\partial^2 f}{\\partial x^2} = \\bm{A^TA} $$\n",
    "\n",
    "As $ A^TA \\succeq 0 $ because it is a squared term, then $ \\bigtriangledown^2 f \\succeq 0$, so $f(x)$ is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 (8%)\n",
    "\n",
    "1.  Prove that if $f(\\boldsymbol{x})$ is convex, then $g(\\boldsymbol{x}) := f(\\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b})$ is convex. \n",
    "        \n",
    "2.  Prove that any norm $\\lVert \\cdot \\rVert$ on $\\mathbb{R}^m$ is convex. \n",
    "        \n",
    "3. Let $$\\begin{align*}\n",
    "            f(\\boldsymbol{x}) := \\max_{i=1,\\cdots,k}~ \n",
    "            \\lVert  \\boldsymbol{A}^{(i)} \\boldsymbol{x} + \\boldsymbol{b}^{(i)} \\rVert, \n",
    "        \\end{align*}$$\n",
    "    where $\\boldsymbol{A}^{(i)} \\in \\mathbb{R}^{m \\times n}$, $\\boldsymbol{b}^{(i)} \\in \\mathbb{R}^m$, and $\\lVert \\cdot \\rVert$ is a norm on $\\mathbb{R}^m$. Indicate whether $f(\\cdot)$ is convex or not. Prove your claim. \n",
    "        \n",
    "4. Let $$\\begin{align*}\n",
    "            f(\\boldsymbol{x}) = \\sum_{i=1}^{r} |x|_{[i]},\n",
    "        \\end{align*}$$\n",
    "    where $\\vert x \\vert_{[i]}$ is the $i$ th largest component of $|x_1|, \\cdots, |x_n|$. Decide whether $f(\\cdot)$ is convex or not. Prove your claim. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "We can use the definition of convexity:\n",
    "\n",
    "Let $ \\lambda \\in [0,1] $\n",
    "\n",
    "$$ g(\\lambda x + (1-\\lambda)y) = f(A(\\lambda x + (1-\\lambda)y)+b) $$\n",
    "\n",
    "$$ = f(\\lambda(Ax+b)+(1-\\lambda)(Ay+b)) $$\n",
    "\n",
    "$$ \\leq \\lambda f(Ax+b) + (1-\\lambda)f(Ay+b) = \\lambda g(x) + (1-\\lambda)g(y) $$\n",
    "\n",
    "\n",
    "2.\n",
    "The domain of $ \\R^m $ is a convex set.\n",
    "\n",
    "Let $ \\lambda \\in [0,1] $:\n",
    "\n",
    "$$ \\|\\lambda x + (1-\\lambda)y\\| \\leq \\|\\lambda x \\| + \\|(1-\\lambda)y\\| = \\lambda \\|x\\| + (1-\\lambda)\\|y\\| $$ \n",
    "\n",
    "where we have used the properties of the norm to show the definition of convexity.\n",
    "\n",
    "3.\n",
    "$ f(x) := max_{i=1,..,k}\\|\\bm{A}^{(i)}\\bm{x} + \\bm{b}^{(i)}\\| $\n",
    "\n",
    "$ f(.) $ is convex because of the following reasoning:\n",
    "\n",
    "$ g(x) = \\|\\bm{Ax} + \\bm{b}\\| $ is convex as proven earlier.\n",
    "\n",
    "$ f(x) $ is in the feasible set of $ g(x) $.\n",
    "\n",
    "The feasible set of a convex optimisation problem is convex.\n",
    "\n",
    "Therefore, $ f(.) $ is convex.\n",
    "\n",
    "4.\n",
    "$ f(x) = \\Sigma^r_{i=1} |x|_{[i]} $, where $|x|_{[i]} $ is the i'th largest component of $ |x_1| $, ..., $|x_n|$.\n",
    "\n",
    "We can prove the function is convex using the 2nd derivative property:\n",
    "\n",
    "$$ \\frac{df}{dx} = \\Sigma_{i=1}^r 1 $$\n",
    "\n",
    "$$ \\frac{d^2f}{dx^2} = \\bm{0} $$\n",
    "\n",
    "Therefore, $ \\frac{d^2f}{dx^2} \\succeq 0 $.\n",
    "\n",
    "Therefore, $ f(x) $ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Search\n",
    "\n",
    "### 6 (6%)\n",
    "Let $\\boldsymbol{x} \\in \\mathbb{R}^2$ and \n",
    "$$\\begin{align*}\n",
    "        f(\\boldsymbol{x}) = 3 |x_1| +  |x_2|.\n",
    "\\end{align*}$$\n",
    "Consider the point $\\boldsymbol{y} = [0,1]^{\\mathsf{T}}$. \n",
    "\n",
    "1.  Show that $\\boldsymbol{g} = [3,1]^{\\mathsf{T}} \\in \\partial f(\\boldsymbol{y})$. \n",
    "2.  Let $\\tau \\in (0,1)$, find the closed form for \n",
    "    $$\\begin{align*}\n",
    "        f(\\boldsymbol{y} - \\tau \\boldsymbol{g} ).\n",
    "    \\end{align*}$$\n",
    "3. Comment on whether $-\\boldsymbol{g}$ is a descent direction or not.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "$ \\partial f(\\bm{y}) = \\{-3,3\\} + 1 $\n",
    "\n",
    "Setting $ y_1 = 0^+ $, Then $ \\partial f(\\bm{y}) = [3, 1]^T = \\bm{g} $\n",
    "\n",
    "2.\n",
    "$$ f(\\bm{y}- \\tau \\bm{g}) = f([0,1]^T - \\tau [3,1]^T) $$ \n",
    "$$ = 3|-3\\tau| + |1-\\tau| $$\n",
    "$$ = 9\\tau + \\begin{cases} 1-\\tau, & \\text{if $ 0 < \\tau \\leq 1 $}.\\\\ \\tau-1, & \\text{$ \\tau \\geq 1 $}. \\end{cases} $$\n",
    "$$ = \\begin{cases} 1-8\\tau, & \\text{if $ 0 < \\tau \\leq 1 $}.\\\\ 10\\tau-1, & \\text{$ \\tau \\geq 1 $}. \\end{cases} $$\n",
    "\n",
    "3.\n",
    "$ -g $ is therefore a descent direction when $ 0 < \\tau < 1/8 $ or $ \\tau \\geq 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 (10%)\n",
    "\n",
    "The following is the famous Wolfe's example, which shows that gradient descent method may not converge to a local optimal point.\n",
    "Let $\\boldsymbol{x}\\in \\mathbb{R}^2$ and \n",
    "$$\\begin{align*}\n",
    "        f(\\boldsymbol{x}) = \\begin{cases}\n",
    "            5(9x_1^2 + 16 x_2^2)^{1/2} & \\text{if}~ x_1 > |x_2|, \\\\\n",
    "            9x_1 + 16|x_2| & \\text{if}~ x_1 \\le |x_2|.\n",
    "        \\end{cases}\n",
    "    \\end{align*}$$\n",
    "Suppose that $\\boldsymbol{x}^0 = [16/9,1]^{\\mathsf{T}}$. Consider exact line search where \n",
    "    $$\\begin{align*}\n",
    "        \\boldsymbol{x}^{l+1} = \\boldsymbol{x}^l - t^{l+1} \\nabla f(\\boldsymbol{x}^l),\n",
    "    \\end{align*}$$\n",
    "    where \n",
    "    $$\\begin{align*}\n",
    "        t^{l+1} = \\arg~ \\min_t~ f(\\boldsymbol{x}^l - t \\nabla f(\\boldsymbol{x}^l)).\n",
    "    \\end{align*}$$\n",
    "\n",
    "\n",
    "1. Draw the contours of $f(\\boldsymbol{x})$ in the region $-2 \\le x_1 \\le 2$ and $-2 \\le x_2 \\le 2$.\n",
    "2. Is the point $\\boldsymbol{x} = [0,0]^{\\mathsf{T}}$ optimal? Why?\n",
    "3. Find the closed form of $\\nabla f(\\boldsymbol{x})$ in the region where $x_1 > |x_2|$.\n",
    "4.  Find $t^1$ and $\\boldsymbol{x}^1$. \n",
    "5. Find $t^2$ and $\\boldsymbol{x}^2$.\n",
    "6. Use mathematical induction, find $t^l$ and $\\boldsymbol{x}^l$. It can be concluded that $\\boldsymbol{x}^l \\rightarrow [0,0]^{\\mathsf{T}}$ as $l \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Gradient Method\n",
    "\n",
    "### 8 (12%)\n",
    "For a given function $f$ and a given constant $\\gamma >0$, the proximal operator is given by\n",
    "\n",
    "$$\\begin{equation*}\n",
    "    \\bm{x}^{\\star} = \\text{Prox}_{\\gamma f}(\\bm{z}) := \\arg~ \n",
    "    \\min_{\\bm{x}}~ f(\\bm{x}) \n",
    "    + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2.\n",
    "\\end{equation*}$$\n",
    "\n",
    "Find the closed form of the proximal operator $\\text{Prox}_{\\gamma f}(\\bm{z})$ for the following functions $f$. Give the key steps for your derivation.  \n",
    "1. $f(\\bm{x}) = \\delta( \\bm{x} \\le u ) $ (each element of $x$ is at most $u$). \n",
    "2. $f(\\bm{x}) = \\delta( l \\le \\bm{x} \\le u ) $ (each element of $x$ is in $[l,u]$).\n",
    "3. $f(\\bm{x}) = \\delta( \\lVert \\bm{x} \\rVert_{\\infty} \\le a ) $. (See Lecture Notes Example 5.1 for the definition of $\\lVert \\cdot \\rVert_{\\infty}$). \n",
    "4. $f(\\bm{x}) = \\frac{1}{2} \\lVert \\bm{A} \\bm{x} - \\bm{b} \\rVert^2 $. \n",
    "5. $f(\\bm{x}) = \\lVert \\bm{x} \\rVert$. \n",
    "6. $f(\\bm{x}) = \\delta( \\lVert \\bm{x} \\rVert \\le a )$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "$ \\delta (\\bm{x} \\leq u) $ is a closed convex set. Therefore, it is an indicator function such that $ \\bm{prox}_{I_C}(z) = min_x \\frac{1}{2}\\|x-z\\|_2^2 + I_C(x) = min_{x \\in C}\\frac{1}{2\\gamma}\\|x-z\\|_2^2$, which is the projection of z onto $ \\{x | x \\leq u \\}. $\n",
    "\n",
    "2.\n",
    "$ \\delta (l \\leq \\bm{x} \\leq u) $ is a closed convex set. Therefore, it is an indicator function such that $ \\bm{prox}_{I_C}(z) = min_x \\frac{1}{2}\\|x-z\\|_2^2 + I_C(x) = min_{x \\in C}\\frac{1}{2\\gamma}\\|x-z\\|_2^2$, which is the projection of z onto $ \\{x | l \\leq x \\leq u \\} $. \n",
    "\n",
    "3.\n",
    "$ \\| x \\|_\\infty \\leq a => max(|x_1|, |x_2|, ..., |x_n|) \\leq a $.\n",
    "Using Moreau decomposition: $ z = \\bm{prox}_f(z) + \\bm{prox}_f^*(z) $, where $ f^*(x) = sup_y(x^Ty - f(y)) $, i.e. the convex conjugate. \n",
    "\n",
    "\n",
    "$ f(x) = \\|x\\|_\\infty $ and $ f^*(x) = I_{\\|x\\|_1 \\leq a}(x) $, where $ I_{\\|.\\|} $ is the indicator function, i.e. in this case \n",
    "$$ 1_S(x) = \\begin{cases} 0, & \\text{if $ x \\leq a $}.\\\\ \\infty, & \\text{otherwise}. \\end{cases} $$\n",
    "$$ \\bm{prox}_f(z) = z - \\bm{prox}_{f^*}(z) $$\n",
    "$$ = z - min_z(1_{\\|z\\|_1 \\leq 1} + \\|x-z\\|_2^2) $$\n",
    "Therefore, $ \\bm{prox}_{\\gamma f}(z) = z -  \\bm{Proj}_{\\|x\\| \\leq a}(x)$\n",
    "\n",
    "4.\n",
    "As $ f(x) $ is differentiable, we can find the derivative and set = 0\n",
    "$$ \\bm{A^T}(\\bm{Ax}-\\bm{b}) + \\frac{1}{\\gamma}(x-z) = 0 $$\n",
    "$$ \\bm{Prox}_{\\gamma f}(z) = \\gamma (\\bm{A^TAx-A^Tb}) - x $$\n",
    "\n",
    "\n",
    "5.\n",
    "$ \\bm{prox}_{\\gamma f}(z) = z - \\gamma\\bm{prox}_{f^*/\\gamma}(z/\\gamma) = z - \\gamma \\bm{Proj}_B(z/\\gamma) $, where $ \\bm{B} $ is the unit ball of the dual norm. We can use the projection operator as $ \\|.\\| $ was proved to be a convex set Question 5.2, thus this form is a special case of the proximal operator. \n",
    "\n",
    "6.\n",
    "This is similar to (5), however it uses an indicator function such that:\n",
    "$$ \\begin{cases} 0, & \\text{if $ x \\leq a / \\gamma $}.\\\\ \\infty, & \\text{otherwise}. \\end{cases} $$\n",
    "Thus, it is a projection onto a ball of radius a:\n",
    "$ \\bm{prox}_{\\gamma f}(z) = z - \\gamma \\bm{Proj}_{aB}(z/\\gamma) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 (14%)\n",
    "Suppose that we know how to compute the proximal operator $\\text{Prox}_{\\gamma g}(\\bm{z})$. Find the form of the proximal operator $\\text{Prox}_{\\gamma f}(\\bm{z})$ in terms of $\\text{Prox}_{\\gamma g}(\\bm{z})$ for the following functions $f$. Give the key steps for your derivation. \n",
    "\n",
    "1.  $f(\\bm{x}) = g(\\bm{x}) + \\frac{\\rho}{2} \\lVert \\bm{x} - \\bm{b} \\rVert^2$ where $\\rho > 0$. \n",
    "2.  $f(\\bm{x}) = g(\\bm{x}) + \\langle \\bm{a},\\bm{x} \\rangle$.\n",
    "3.  $f(\\bm{x}) = a g(\\bm{x}) + b$ where $a>0$.\n",
    "4.  $f(\\bm{x}) = g(\\bm{x} + b)$.\n",
    "5.  $f(\\bm{x}) = g(a\\bm{x})$ where $a>0$.\n",
    "6.  $f(\\bm{x}) = g(\\bm{A}\\bm{x})$ where $\\bm{A}$ is orthonormal, i.e., $\\bm{A}^{\\mathsf{T}}\\bm{A} = \\bm{A} \\bm{A}^{\\mathsf{T}} = \\bm{I}$.\n",
    "7.  $f(\\bm{x}) = g(a\\bm{A}\\bm{x})$ where $a>0$ and $\\bm{A}$ is orthonormal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "This is a regularization problem, such that:\n",
    "$$ \\bm{Prox}_{\\gamma f}(z) = \\bm{Prox}_{\\frac{\\gamma}{1+\\gamma \\rho} g}((\\frac{\\gamma}{1+\\gamma \\rho})\\bm{z}+(\\frac{\\rho \\gamma}{1+\\gamma \\rho}b)) $$\n",
    "\n",
    "2.\n",
    "This is affine addition, such that:\n",
    "$$ \\bm{Prox}_{\\gamma f}(z) = \\bm{Prox}_{\\gamma g}(z-\\gamma a) $$\n",
    "\n",
    "3. \n",
    "This is postcomposition, such that:\n",
    "$$ \\bm{Prox}_{\\gamma f}(z) = \\bm{Prox}_{\\gamma ag}(z) $$\n",
    "\n",
    "4. \n",
    "This is precomposition, such that:\n",
    "$$ \\bm{Prox}_{\\gamma f}(z) = \\bm{Prox}_{\\gamma ag}(z) $$\n",
    "\n",
    "5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###   10 MRI Compressed Sensing Recovery via Proximal Gradient (24%)\n",
    "\n",
    "Let $\\bm{X}$ be an MRI image. A typical MRI image is sparse under wavelet transform. An MRI machine takes samples in the frequency domain. \n",
    "\n",
    "\n",
    "*  We verify that an MRI image is sparse under wavelet transform as follows. (6%)\n",
    "\n",
    "    1.  Use the package `Images` to load `MRI_of_Human_Brain.jpg`. Show the MRI image that you load. Denote the image by `X0`. \n",
    "\n",
    "    2.  Use the package `Wavelets` to perform 2D Daubechies wavelet transform of the MRI image. Show the wavelet coefficients as an image. (See `Github>Wavelets.jl/example/transform2d` for an example.) Denote the wavelet coefficients by `W0`.\n",
    "    \n",
    "    3.  Keep 20% wavelet coefficients that are of the largest magnitudes and set the rest wavelet coefficients to zero. Denote the resulting 'truncated' wavelet coefficients by `W1`. Perform inverse 2D Daubechies wavelet transform to obtain an image `X1`. This image should look like the image `X0` with minor quality loss. \n",
    "\n",
    "    \n",
    "* Fourier domain compressed sensing: (6%)\n",
    "\n",
    "    4.  Use the package `FFTW` to perform 2D Fourier transform of `X0`. Denote the obtained Fourier coefficients by `F0`. \n",
    "\n",
    "    5. Set the random seed to 0. Generate the random sampling index set by using `StatsBase.sample` and denote it by $\\Omega$ such that the size of $\\Omega$ is 30% of the size of `X0`. \n",
    "\n",
    "    6.  Let $y = \\mathcal{P}_{\\Omega} ( \\mathcal{F}(X0))$. It is our MRI measurement vector. \n",
    "\n",
    "\n",
    "* Compressed sensing recovery. We are going to use proximal gradient method to solve the optimization problem. We are going to denote the recovered image by `Xhat`. (12%) \n",
    "\n",
    "    $$\\begin{align*}\n",
    "        & \\min_{X}~ F(X) := \\lambda \\lVert \\mathcal{W}(X) \\rVert_1 + \\frac{1}{2} \\lVert y - \\mathcal{P}_{\\Omega} (\\mathcal{F}(X)) \\rVert_2^2.\n",
    "    \\end{align*}$$ \n",
    "    \n",
    "    7.  Use the results that you obtained in coursework problem related to proximal operators to obtain the form of the proximal operator for the function $g(X) = \\lambda \\lVert \\mathcal{W}(X) \\rVert_1$. \n",
    "\n",
    "    8.  Write the objective function $F(X) = f(X) + g(X)$ where $f(X)$ is differentiable. Find the closed form for $\\nabla f(X)$ (Recall the adjoint of linear operator $\\mathcal{A}$ is denoted by $\\mathcal{A}^*$). \n",
    "    \n",
    "    9.  Implement the proximal gradient method to recover the image from $y$. In your codes, use comments to indicate clearly which part is to compute $\\nabla f(X)$, which part is to perform proximal operator, and which part is to find a valid step size. \n",
    "\n",
    "\n",
    "    10. Test the performance. Now find a good value of $\\lambda$ so that the recovery result looks nice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0de548ffb181fe2a01b2ed5f402e6bb6e6ccf2641efffae0817d36d41e1e10e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
